{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-baseline",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from layers import BilinearUpSampling2D\n",
    "from loss import depth_loss_function\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import glob\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    import numpy as np\n",
    "    (654, 480, 640)\n",
    "    normalized = (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    normalized = np.clip(normalized, 0.001, 1, out=normalized)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def load_images(image_files):\n",
    "    loaded_images = []\n",
    "    for file in image_files:\n",
    "        x = np.asarray(Image.open(file), dtype=float)\n",
    "        loaded_images.append(x)\n",
    "    return np.stack(loaded_images, axis=0)\n",
    "\n",
    "def DepthNorm(x, maxDepth):\n",
    "    return maxDepth / x\n",
    "\n",
    "def predictue(model, images, minDepth=0, maxDepth=255, batch_size=2):\n",
    "    # Support multiple RGBs, one RGB image, even grayscale \n",
    "    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n",
    "    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n",
    "    # Compute predictions\n",
    "    predictions = model.predict(images, batch_size=batch_size)\n",
    "    # Put in expected range\n",
    "    return normalize(predictions)\n",
    "\n",
    "def compute_errors(gt, pred):\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "    log_10 = (np.abs(np.log10(gt) - np.log10(pred))).mean()\n",
    "    return a1, a2, a3, abs_rel, rmse, log_10\n",
    "\n",
    "def evaluate_ue(model, rgb, depth, batch_size=6, minDepth=10, maxDepth = 1000, verbose=False):\n",
    "    N = len(rgb)\n",
    "\n",
    "    bs = batch_size\n",
    "\n",
    "    predictions = []\n",
    "    testSetDepths = []\n",
    "    \n",
    "    for i in range(N//bs):    \n",
    "        x = rgb[(i)*bs:(i+1)*bs,:,:,:]\n",
    "        \n",
    "        # Compute results\n",
    "        true_y = depth[(i)*bs:(i+1)*bs,:,:]\n",
    "        pred_y = scale_up(2, predictue(model, x/255, minDepth=minDepth, maxDepth=maxDepth, batch_size=bs)[:,:,:,0])\n",
    "        \n",
    "        # Test time augmentation: mirror image estimate\n",
    "        pred_y_flip = scale_up(2, predictue(model, x[...,::-1,:]/255, minDepth=minDepth, maxDepth=maxDepth, batch_size=bs)[:,:,:,0])\n",
    "\n",
    "        \n",
    "        # Compute errors per image in batch\n",
    "        for j in range(len(true_y)):\n",
    "            predictions.append(   (0.5 * pred_y[j]) + (0.5 * np.fliplr(pred_y_flip[j]))   )\n",
    "            testSetDepths.append(   true_y[j]   )\n",
    "\n",
    "    predictions = np.stack(predictions, axis=0)\n",
    "    testSetDepths = np.stack(testSetDepths, axis=0)\n",
    "\n",
    "    e = compute_errors(predictions, testSetDepths)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n",
    "        print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n",
    "\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-emerald",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-planet",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelname = 'morse13k.h5'\n",
    "custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': depth_loss_function}\n",
    "uemodel = load_model(modelname, custom_objects=custom_objects, compile=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prairie",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "source = 'E:/Dataset/Dataset/Ablation/'\n",
    "#source = './test_data/same_domain/'\n",
    "depthlist = glob.glob(source + 'Depth/*_final.png')\n",
    "rgblist = glob.glob(source + 'Same_domain/*_final.png')\n",
    "\n",
    "uedepth = load_images(depthlist)\n",
    "uergb = load_images(rgblist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-viking",
   "metadata": {},
   "source": [
    "# Compute Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "scoreaverage = []\n",
    "for i, (img, label) in enumerate(zip(uergb, uedepth)):\n",
    "    \n",
    "    pred = predictue(uemodel, img).squeeze()\n",
    "    scaled = resize(pred, (480,640), order=1, preserve_range=True, mode='reflect', anti_aliasing=True)\n",
    "    error = compute_errors(label,scaled)\n",
    "    if error[0] > 0 : \n",
    "        count+=1\n",
    "        scoreaverage.append(error)\n",
    "        print(i, end='\\r')\n",
    "        \n",
    "scoreaverage = np.array(scoreaverage)\n",
    "print(scoreaverage.mean(0))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(img/255, cmap='magma')\n",
    "plt.show()\n",
    "plt.axis('off')\n",
    "plt.imshow(scaled, cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(Image.open(depthlist[1])).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-comparative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
